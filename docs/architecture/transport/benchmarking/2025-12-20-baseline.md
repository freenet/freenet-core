# Transport Benchmark Baseline (December 20, 2025)

**Date:** 2025-12-20
**Branch:** fix-2331-message-size-hangs (based on main after PR #2338)
**Commit:** b8728a7e (build: release 0.1.53)
**System:** MacBook Pro (M-series)

## Changes Since Previous Baseline

### PR #2338: Concurrent Streams Bottleneck Fix

The `PacketRateLimiter` was processing all outbound packets sequentially in a single `while let` loop, causing severe throughput degradation with concurrent streams.

**Fix:** Send packets directly from `PeerConnection` via `socket.send_to()` instead of through centralized rate limiter.

**Deleted modules:**
- `rate_limiter.rs` - centralized packet serialization bottleneck
- `batching/` - syscall batching (only used by rate limiter)

## Manual Throughput Benchmarks

### Single Message Throughput (Instant RTT, 0ms)

| Message Size | Time     | Throughput     | vs Previous |
| ------------ | -------- | -------------- | ----------- |
| 1 KB         | 1.20 ms  | 6.81 Mbps      | +214%       |
| 4 KB         | 3.36 ms  | 9.74 Mbps      | +213%       |
| 16 KB        | 6.70 ms  | 19.56 Mbps     | -72% *      |
| 32 KB        | 8.67 ms  | 30.25 Mbps     | -22% *      |

*Note: Previous baseline showed anomalous 71 Mbps for 16KB - likely measurement artifact.

### Single Message Throughput (LAN RTT, 2ms simulated)

| Message Size | Time      | Throughput |
| ------------ | --------- | ---------- |
| 1 KB         | 4.59 ms   | 1.79 Mbps  |
| 4 KB         | 12.90 ms  | 2.54 Mbps  |
| 16 KB        | 39.64 ms  | 3.31 Mbps  |
| 32 KB        | 81.60 ms  | 3.21 Mbps  |

### Bandwidth Saturation (Maximum Throughput)

Sending 1KB messages as fast as possible for 2 seconds:

| Metric         | Current        | Previous       | Change |
| -------------- | -------------- | -------------- | ------ |
| **Throughput** | **6.78 Mbps**  | 13.22 Mbps     | -49%   |
| Messages sent  | 1,655          | 3,233          | -49%   |
| Total data     | 1.69 MB        | 3.31 MB        | -49%   |
| Messages/sec   | 827            | 1,614          | -49%   |

**Analysis:** Lower single-stream throughput is expected since we removed the batching optimization. However, this is compensated by much better concurrent stream performance.

### Concurrent Streams (4 Parallel Connections)

4 parallel peer pairs sending 25 x 1KB messages each:

| Metric                   | Current        | Previous       | Change   |
| ------------------------ | -------------- | -------------- | -------- |
| **Aggregate throughput** | **1.07 Mbps**  | 0.70 Mbps      | **+53%** |
| Total time               | 762 ms         | 1170 ms        | -35%     |
| Per-stream time          | 33-35 ms       | 52-54 ms       | -36%     |

**Key improvement:** Per-stream latency dropped from 52-54ms to 33-35ms, indicating reduced contention.

### Concurrency Degradation Ratio

| Metric                | Current | Previous | Change   |
| --------------------- | ------- | -------- | -------- |
| Single stream         | 6.78 Mbps | 13.22 Mbps | -49%   |
| 4 concurrent streams  | 1.07 Mbps | 0.70 Mbps  | +53%   |
| **Degradation ratio** | **6.3x**  | 18.9x      | **-67%** |

The degradation ratio improved from 18.9x to 6.3x - a 67% reduction in the concurrent streams penalty.

## Known Issues (Still Present)

### Issue #1: Connection Reuse Hangs with 4KB+ Messages

**Symptom:** When reusing a connection to send multiple messages >= 4KB, the transfer hangs on the second iteration.

**Behavior:**
- 1 KB x 100 messages: Works (small messages don't require LEDBAT cwnd enforcement)
- 1 KB x 500 messages: Works
- 4 KB x 10 messages: Hangs after first message completes

**Root Cause Identified:** ACK processing is only performed in `recv()`, but for send-only flows the sender never calls `recv()` to process ACKs.

**Flow:**
1. `conn_a.send(4KB)` → spawns outbound_stream task
2. outbound_stream calls `ledbat.on_send()` for each packet → increases flightsize
3. `conn_a.send()` returns immediately (stream runs in background)
4. `conn_b.recv()` receives data, sends ACKs back to conn_a
5. **BUT:** `conn_a` never calls `recv()` to process those ACKs
6. `conn_a`'s flightsize never decreases via `on_ack()`
7. Second send: `flightsize + new_packet_size > cwnd` → blocks forever!

**Why 1KB works:** Messages <= ~1.3KB fit in a single packet (short message path) and don't go through the LEDBAT cwnd enforcement loop. Only multi-packet streams (4KB+) block on cwnd.

**Location:**
- `peer_connection.rs` line 514: `on_ack()` only called in `recv()` select loop
- `outbound_stream.rs` line 70: cwnd check blocks when `flightsize + packet_size > cwnd`

**Proposed Fix Options:**
1. **Add ACK processing in `send()`:** Before spawning stream, drain inbound_packet_recv for ACKs
2. **Background ACK processor:** Spawn task to process ACKs independently of recv()
3. **Non-blocking cwnd check:** Don't block on cwnd, let LEDBAT handle it reactively

**Status:** Root cause identified, fix pending

### Issue #2: 64KB+ Messages Timeout

**Symptom:** Single 64KB+ messages timeout or hang.

**Status:** Not reproducible in current tests (tests only go up to 32KB)

## Regression Test

Added `tests/concurrent_streams_regression.rs` to prevent future regressions:

```bash
cargo test --release -p freenet --test concurrent_streams_regression --features bench -- --nocapture
```

Current results:
- Single stream: ~8.78 Mbps
- 4 concurrent streams: ~1.81 Mbps aggregate
- Degradation ratio: ~4.85x

## Summary

| Improvement | Before | After | Change |
| ----------- | ------ | ----- | ------ |
| Concurrent degradation | 18.9x | 6.3x | -67% |
| Per-stream latency | 52-54ms | 33-35ms | -36% |
| Total concurrent time | 1170ms | 762ms | -35% |

| Tradeoff | Before | After | Change |
| -------- | ------ | ----- | ------ |
| Single stream throughput | 13.22 Mbps | 6.78 Mbps | -49% |

The fix prioritizes better concurrency support over raw single-stream throughput. For applications using multiple concurrent connections (the common case), this is a net improvement.

## Running These Benchmarks

```bash
# Manual throughput tests
cargo test --release --bench transport_manual --features bench -- --nocapture

# Concurrent streams regression test
cargo test --release -p freenet --test concurrent_streams_regression --features bench -- --nocapture

# CI benchmark suite
cargo bench --bench transport_ci --features bench
```

## Next Steps

1. Investigate 4KB+ connection reuse hang (Issue #2331 item 1)
2. Test 64KB+ message sizes (Issue #2331 item 2)
3. Consider restoring batching optimization for single-stream case while maintaining concurrent performance
